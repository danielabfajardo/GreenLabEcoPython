# Research Question 2 hypothesis testing

### Setup
```{r, results='hide', message=FALSE}
install.packages(c("tidyverse", "ggplot2", "bestNormalize", "ARTool", "lmerTest", "xtable"))
library(tidyverse)
library(ggplot2)
library(bestNormalize)
library(ARTool)
library(lmerTest)
library(xtable)
```

## Data Loading 

```{r}
# load the dataset from data/run_table
run_data <- read_csv("data/run_table.csv")
```
```{r}
# Inspect the dataset structure
glimpse(run_data)
head(run_data)
summary(run_data)
```
```{r}
# Load data and handle non-standard column names
run_data <- run_data %>%
  # Use backticks for non-standard column name `__done`
  filter(`__done` == "DONE") %>%
  mutate(
    algorithm = as_factor(algorithm),
    library = as_factor(library),
    dataset = as_factor(dataset),
    process = as_factor(process),
    group_key = paste(library, algorithm, process, dataset, sep = "_"),
    trial = as_factor(str_extract(`__run_id`, "(?<=rep)\\d+"))
  )


run_data_training <- run_data %>% filter(process == "training")
run_data_inference <- run_data %>% filter(process == "inference")

tail(run_data %>% select(`__run_id`, process, trial))
dim(run_data_training)
```

dim(run_data_inference)
[1] 240  18
dim(run_data_training)
[1] 240  18

## Data Normality

```{r}
# Run Shapiro-Wilk for each group
normality_results_t <- run_data_training %>%
  group_by(library, dataset) %>%
  summarise(
    shapiro_p = shapiro.test(energy_j)$p.value,
    .groups = "drop"
  )

normality_results_i <- run_data_inference %>%
  group_by(library, dataset) %>%
  summarise(
    shapiro_p = shapiro.test(energy_j)$p.value,
    .groups = "drop"
  )

normality_results_t
```

p < 0.05 for each library*dataset combination - reject normality (data is not normal).

### Normalization

```{r}
bn_t <- bestNormalize(run_data_training$energy_j)
run_data_training$norm_energy_usage <- bn_t$x.t

bn_i <- bestNormalize(run_data_inference$energy_j)
run_data_inference$norm_energy_usage <- bn_i$x.t
```

# Shapiro-Wilk again for normalized values

```{r}
# Histogram of normalized values
ggplot(run_data_training, aes(x = norm_energy_usage)) +
  geom_histogram(bins = 30, fill = "lightgreen", color = "black") +
  theme_minimal() +
  labs(
    title = "Histogram of Normalized Energy Usage",
    x = "Normalized Energy Usage",
    y = "Frequency"
  )

normality_results_norm <- run_data_training %>%
  group_by(library, dataset) %>%
  summarise(
    shapiro_p = shapiro.test(norm_energy_usage)$p.value,
    .groups = "drop"
  )

normality_results_norm
```

For both libraries in large dataset size we can't reject normality.
Because many values are still below 0.05, we proceed with non-parametric tests (Wilcoxon, ART).

## Non-Parametric Tests for RQ2

### Aligned rank transform (ART) ANOVA 

```{r, results='hide', message=FALSE}
library(ARTool)

algorithms <- unique(run_data_training$algorithm) #training, inference

# Function to run ART ANOVA and return results
run_art_analysis <- function(data, response_var, process_label, alg_label) {
formula_str <- paste0(response_var, " ~ library * dataset + Error(trial)")
model <- art(as.formula(formula_str), data = data)
res <- as.data.frame(anova(model))

list(model = model, results = res, process = process_label, algorithm = alg_label, response = response_var)
}

# Run for each combination: process × algorithm × response
art_results <- list()

for (alg in algorithms) {
  # Training Energy
  art_results[[paste0("t_energy_", alg)]] <- run_art_analysis(
  run_data_training %>% filter(algorithm == alg),
  "energy_j",
  "training",
  alg
  )

  # Inference Energy
  art_results[[paste0("i_energy_", alg)]] <- run_art_analysis(
  run_data_inference %>% filter(algorithm == alg),
  "energy_j",
  "inference",
  alg
  )

  # Training Runtime
  art_results[[paste0("t_runtime_", alg)]] <- run_art_analysis(
  run_data_training %>% filter(algorithm == alg),
  "runtime_s",
  "training",
  alg
  )

  # Inference Runtime
  art_results[[paste0("i_runtime_", alg)]] <- run_art_analysis(
  run_data_inference %>% filter(algorithm == alg),
  "runtime_s",
  "inference",
  alg
  )
}



# model_t = art(energy_j ~ library * dataset + Error(trial), data = run_data_training)
# anova_results_t <- as.data.frame(anova(model_t))

# model_i = art(energy_j ~ library * dataset + Error(trial), data = run_data_inference)
# anova_results_i <- as.data.frame(anova(model_i))

```

-   To check if model is correct we simpy run command `summary(model)` and investigate whether all F-values are 0.
F values of ANOVAs on aligned responses not of interest (should all be ~0):
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
      0       0       0       0       0       0 

-   `anova(model)` shows us information about influence of each variable and their interactions.

Training:
                  Error Df Df.res F value     Pr(>F)    
1 library         Withn  1    215 141.110 < 2.22e-16 ***
2 dataset         Withn  2    215  63.930 < 2.22e-16 ***
3 library:dataset Withn  2    215  69.742 < 2.22e-16 ***

Inference:
                  Error Df Df.res F value     Pr(>F)    
1 library         Withn  1    215 114.643 < 2.22e-16 ***
2 dataset         Withn  2    215 354.023 < 2.22e-16 ***
3 library:dataset Withn  2    215  54.511 < 2.22e-16 ***

### Post-hoc analysis

```{r, results='hide', message=FALSE}
# interaction_contrasts_t <- art.con(model_t, "library:dataset")
# summary(interaction_contrasts_t)

# interaction_contrasts_i <- art.con(model_i, "library:dataset")
# summary(interaction_contrasts_i)
```

### Residuals

```{r}
# qqnorm(resid(model_t))
# qqline(resid(model_t))
# shapiro.test(resid(model_t))
```

We can see residuals are not normal, which might be the result that our data doesn't meet the conditional normality assumption, even though it's distribution with no regard to factors looks normal.

## Annotated Tables

```{r}
library(stringr)

make_anova_table <- function(anova_df, caption, label) {
  # Number of tests = number of rows
  n_tests <- nrow(anova_df)

  anova_df <- anova_df %>%
    mutate(
      p.value = `Pr(>F)`,
      p.bonf = p.adjust(p.value, method = "bonferroni")
    )
  anova_df <- anova_df %>%
    mutate(
      Term = case_when(
        Term == "dataset" ~ "Dataset Size",
        Term == "library" ~ "Library",
        Term == "library:dataset" ~ "Library × Dataset Size",
        TRUE ~ Term
      ),
      partial_eta2 = `Sum Sq` / (`Sum Sq` + `Sum Sq.res`),
      p.value = ifelse(`Pr(>F)` < 0.001, "< 0.001", sprintf("%.3f", `Pr(>F)`)),
      p.bonf = ifelse(p.bonf < 0.001, "< 0.001", sprintf("%.3f", p.bonf))
    ) %>%
    select(Term, Df, `F value`, p.bonf, partial_eta2)
  
  xt <- xtable(
    anova_df,
    caption = caption,
    label = label,
    digits = c(0, 0, 0, 3, 3, 3)
  )
  
  # Escape backslashes properly
  names(xt) <- c(
    "\\textbf{Term}",
    "\\textbf{Df}",
    "\\textbf{F value}",
    "\\textbf{$p$-value}",
    "\\textbf{Partial $\\eta^2$}"
  )
  
  print(
    xt,
    include.rownames = FALSE,
    caption.placement = "top",
    sanitize.text.function = identity
  )
}


for (name in names(art_results)) {
  res <- art_results[[name]]
  alg <- res$algorithm
  proc <- res$process
  resp <- ifelse(res$response == "energy_j", "Energy Consumption", "Runtime")

  caption <- paste0(
    "ART ANOVA for ", resp, " (", str_to_title(proc),
    ", Algorithm = ", str_to_title(alg), ")"
  )
  
  # Safe LaTeX label (no spaces or special chars)
  label <- paste0(
    "tab_art_", proc, "_",
    ifelse(res$response == "energy_j", "energy", "runtime"), "_",
    alg
  )
  
  cat("\n\n%### ", caption, "\n")
  
  make_anova_table(
    res$results,
    caption = caption,
    label = label
  )
}


```

### Training
```{r}
# library(xtable)
# library(dplyr)

# anova_df_t <- as.data.frame(anova_results_t) %>%
#   mutate(
#     Term = case_when(
#       Term == "dataset" ~ "Dataset Size",
#       Term == "library" ~ "Library",
#       Term == "library:dataset" ~ "Library × Dataset Size",
#       TRUE ~ Term
#     ),
#     # Compute partial eta squared
#     partial_eta2 = `Sum Sq` / (`Sum Sq` + `Sum Sq.res`),
#     # Format p-values nicely
#     p.value = ifelse(`Pr(>F)` < 0.001, "< 0.001", sprintf("%.3f", `Pr(>F)`))
#   ) %>%
#   select(
#     Term,
#     Df,
#     `F value`,
#     p.value,
#     partial_eta2
#   )

# # Create LaTeX table
# anova_xtable_t <- xtable(
#   anova_df_t,
#   caption = "ART ANOVA for Energy Consumption (Training)",
#   label = "tab:art_training",
#   digits = c(0, 0, 0, 3, 3, 3)
# )

# # Custom column names (with LaTeX math mode for p-value)
# names(anova_xtable_t) <- c("\\textbf{Term}",
#                            "\\textbf{Df}",
#                            "\\textbf{F value}",
#                            "\\textbf{$p$-value}",
#                            "\\textbf{Partial $\\eta^2$}")

# # Print with LaTeX syntax preserved
# print(
#   anova_xtable_t,
#   include.rownames = FALSE,
#   caption.placement = "top",
#   sanitize.text.function = identity
# )
```

### Inference

```{r}
# anova_df_i <- as.data.frame(anova_results_i) %>%
#   mutate(
#     Term = case_when(
#       Term == "dataset" ~ "Dataset Size",
#       Term == "library" ~ "Library",
#       Term == "library:dataset" ~ "Library × Dataset Size",
#       TRUE ~ Term
#     ),
#     # Compute partial eta squared
#     partial_eta2 = `Sum Sq` / (`Sum Sq` + `Sum Sq.res`),
#     # Format p-values nicely
#     p.value = ifelse(`Pr(>F)` < 0.001, "< 0.001", sprintf("%.3f", `Pr(>F)`))
#   ) %>%
#   select(
#     Term,
#     Df,
#     `F value`,
#     p.value,
#     partial_eta2
#   )

# # Create LaTeX table
# anova_xtable_i <- xtable(
#   anova_df_i,
#   caption = "ART ANOVA for Energy Consumption (Inference)",
#   label = "tab:art_inference",
#   digits = c(0, 0, 0, 3, 3, 3)
# )

# # Custom column names (with LaTeX math mode for p-value)
# names(anova_xtable_i) <- c("\\textbf{Term}",
#                            "\\textbf{Df}",
#                            "\\textbf{F value}",
#                            "\\textbf{$p$-value}",
#                            "\\textbf{Partial $\\eta^2$}")

# # Print with LaTeX syntax preserved
# print(
#   anova_xtable_i,
#   include.rownames = FALSE,
#   caption.placement = "top",
#   sanitize.text.function = identity
# )
```